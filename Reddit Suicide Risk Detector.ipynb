{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reddit Suicide Risk Detector: End-to-End Documentation\n",
        "\n",
        "This notebook guides you through building a suicide risk detector for Reddit posts, step by step, with code you can run directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "Install dependencies in your terminal:\n",
        "```
        "pip install pandas scikit-learn nltk praw flask joblib\n",
        "```\n",
        "Download NLTK resources in a notebook cell:\n",
        "```
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Collection\n",
        "\n",
        "Fetch posts from Reddit using PRAW. Replace the credentials with your own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id='YOUR_CLIENT_ID',\n",
        "    client_secret='YOUR_CLIENT_SECRET',\n",
        "    user_agent='YOUR_USER_AGENT'\n",
        ")\n",
        "\n",
        "subreddits = [\n",
        "    'SuicideWatch', 'depression', 'anxiety', 'mentalhealth', 'mentalillness',\n",
        "    'offmychest', 'CasualConversation', 'AskReddit', 'TodayILearned', 'funny'\n",
        "]\n",
        "limit = 100\n",
        "posts = []\n",
        "for subreddit_name in subreddits:\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    for submission in subreddit.new(limit=limit):\n",
        "        posts.append({\n",
        "            'post_id': submission.id,\n",
        "            'subreddit': subreddit_name,\n",
        "            'title': submission.title,\n",
        "            'post_text': submission.selftext\n",
        "        })\n",
        "df = pd.DataFrame(posts)\n",
        "df.to_csv('reddit_posts.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Cleaning\n",
        "\n",
        "Remove duplicates and empty posts to keep the data meaningful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv('reddit_posts.csv')\n",
        "df = df.drop_duplicates(subset=['post_id'])\n",
        "df = df.dropna(subset=['title', 'post_text'])\n",
        "df = df[(df['title'].str.strip() != '') | (df['post_text'].str.strip() != '')]\n",
        "df.to_csv('reddit_posts_cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Labeling\n",
        "\n",
        "Label posts from mental health subreddits as high-risk (1), others as neutral (0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mental_health_subs = [\n",
        "    'SuicideWatch', 'depression', 'anxiety', 'mentalhealth', 'mentalillness', 'offmychest'\n",
        "]\n",
        "df = pd.read_csv('reddit_posts_cleaned.csv')\n",
        "df['label'] = df['subreddit'].apply(lambda x: 1 if x in mental_health_subs else 0)\n",
        "df.to_csv('reddit_posts_labeled.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Text Preprocessing\n",
        "\n",
        "Standardize text: lowercase, remove punctuation, remove stopwords, and lemmatize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^\\\\w\\\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df = pd.read_csv('reddit_posts_labeled.csv')\n",
        "df['clean_text'] = df['title'].fillna('') + ' ' + df['post_text'].fillna('')\n",
        "df['clean_text'] = df['clean_text'].apply(clean_text)\n",
        "df.to_csv('reddit_posts_preprocessed.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Extraction\n",
        "\n",
        "Convert text to TF-IDF features for machine learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "df = pd.read_csv('reddit_posts_preprocessed.csv')\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df['clean_text'])\n",
        "y = df['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Training\n",
        "\n",
        "Train a logistic regression model to classify posts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Evaluation\n",
        "\n",
        "Check how well the model performs on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=['neutral', 'high-risk']))\n",
        "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Saving\n",
        "\n",
        "Save the trained model and vectorizer for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(clf, 'suicide_risk_logreg_model.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Inference Function\n",
        "\n",
        "Predict risk for new Reddit post text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def predict_post_risk(title, post_text):\n",
        "    clf = joblib.load('suicide_risk_logreg_model.pkl')\n",
        "    vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "    text = clean_text(f'{title} {post_text}')\n",
        "    X_new = vectorizer.transform([text])\n",
        "    prediction = clf.predict(X_new)\n",
        "    return 'high-risk' if prediction == 1 else 'neutral'\n",
        "\n",
        "# Example usage:\n",
        "print(predict_post_risk('I feel hopeless', 'I don\\'t know how to go on.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. How to Use the Web Demo\n",
        "\n",
        "Run `app.py` (see README) to launch the web app. Paste a Reddit post URL to get a risk prediction."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
